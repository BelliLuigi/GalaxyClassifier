{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c831bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d162ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, num_filters,num_neurons1, num_neurons2):\n",
    "        super().__init__()\n",
    "        ### Maxpooling layer!!!!\n",
    "        self.rgb = 1\n",
    "        self.input_size = 324\n",
    "        self.num_labels = 37\n",
    "        self.loss_dict = { 'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "\n",
    "\n",
    "        stride = 2\n",
    "        kernel_size = 3\n",
    "        kernel_size_pool = 2\n",
    "        ## convolutional layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(self.rgb, num_filters[0], kernel_size=kernel_size)])\n",
    "        self.convs.append(nn.BatchNorm2d(num_filters[0]))\n",
    "        output_size = (self.input_size - kernel_size + stride) // (stride*kernel_size_pool)\n",
    "        for i in range(1,n_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=kernel_size)) # num filters are the number of channel the conv layer otputs.\n",
    "            self.convs.append(nn.BatchNorm2d(num_filters[i]))\n",
    "            output_size = (output_size - kernel_size + stride) // (stride*kernel_size_pool) #padding 0, dilation = 1\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size_pool)\n",
    "        #self.convs.append(nn.dropout(p= value)) ## to be added in the future to test claims of BatchnOrm paper\n",
    "        self.out_feature = num_filters[-1] *output_size * output_size # output size of the last conv layer\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons1) # fully connected layer\n",
    "        # dropout here if you want\n",
    "        self.fc2 = nn.Linear(num_neurons1, num_neurons2)\n",
    "        #dropout here if u want\n",
    "        self.fc3 = nn.Linear(num_neurons2, self.num_labels) \n",
    "\n",
    "    def init_weights(self,  mode, n_conv_layers):\n",
    "        if mode == 'relu': # perchÃ¨ kaiming normal e non uniform??1\n",
    "            for i in range(0, n_conv_layers*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity=mode)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=mode)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=mode)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0)\n",
    "        elif mode == 'leaky_relu':\n",
    "            for i in range(0, n_conv_layers*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, a = 0.01, nonlinearity=mode)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=mode)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=mode)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0) \n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.constant_(self.fc3.bias,0)\n",
    "        return print('weights initialized with {}'.format(mode))        \n",
    "        \n",
    "\n",
    "    def forward(self,x, FUNZIONEDIATTIVAZIONE):\n",
    "        for i in range(0, len(self.convs),2):\n",
    "            x = self.pool(FUNZIONEDIATTIVAZIONE(self.convs[i](x)))\n",
    "            x = self.convs[i+1](x) # batch norm layer\n",
    "        x = torch.flatten(x,1) # flatten operation -> 1 dimensional\n",
    "        x = FUNZIONEDIATTIVAZIONE(self.fc1(x)) # apply relu al'output dei fully connected\n",
    "        x = FUNZIONEDIATTIVAZIONE(self.fc2(x)) # idem sopra\n",
    "        x = self.fc3(x) # output di fc3, 37 neuroni -> 37 classi ideally \n",
    "        ## MAPPING ALSO HERE\n",
    "        return x\n",
    "    \n",
    "    def set_rgb(self,bool):\n",
    "        if bool:\n",
    "            self.rgb = 3\n",
    "        else:\n",
    "            self.rgb = 1\n",
    "        return self.rgb\n",
    "    \n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        train = self.__getstate__()['training']\n",
    "        print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a06e03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_conv_layers, num_filters,num_neurons1, num_neurons2):\n",
    "\n",
    "prova = GalaxyNet(2,[3,6,9],50, 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a44d75c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova.set_rgb(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a51cea",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [20, 10, 3, 3], expected input[10, 2, 300, 300] to have 10 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m m = nn.MaxPool2d(kernel_size=\u001b[32m2\u001b[39m)\n\u001b[32m      3\u001b[39m n = nn.Conv2d(\u001b[32m10\u001b[39m,\u001b[32m20\u001b[39m,\u001b[32m3\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m x = m(n(tensor))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight, \u001b[38;5;28mself\u001b[39m.bias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    454\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    455\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    457\u001b[39m                 \u001b[38;5;28mself\u001b[39m.padding, \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [20, 10, 3, 3], expected input[10, 2, 300, 300] to have 10 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(10,2,300,300)\n",
    "m = nn.MaxPool2d(kernel_size=2)\n",
    "n = nn.Conv2d(10,20,3)\n",
    "\n",
    "x = m(n(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f62fc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDocstring:\u001b[39m\n",
      "randn(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "\n",
      "\n",
      "Returns a tensor filled with random numbers from a normal distribution\n",
      "with mean `0` and variance `1` (also called the standard normal\n",
      "distribution).\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{N}(0, 1)\n",
      "\n",
      "For complex dtypes, the tensor is i.i.d. sampled from a `complex normal distribution`_ with zero mean and\n",
      "unit variance as\n",
      "\n",
      ".. math::\n",
      "    \\text{out}_{i} \\sim \\mathcal{CN}(0, 1)\n",
      "\n",
      "This is equivalent to separately sampling the real :math:`(\\operatorname{Re})` and imaginary\n",
      ":math:`(\\operatorname{Im})` part of :math:`\\text{out}_i` as\n",
      "\n",
      ".. math::\n",
      "    \\operatorname{Re}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2}),\\quad\n",
      "    \\operatorname{Im}(\\text{out}_{i}) \\sim \\mathcal{N}(0, \\frac{1}{2})\n",
      "\n",
      "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "\n",
      "\n",
      "Args:\n",
      "    size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "        Can be a variable number of arguments or a collection like a list or tuple.\n",
      "\n",
      "Keyword args:\n",
      "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "    out (Tensor, optional): the output tensor.\n",
      "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "        Default: if ``None``, uses a global default (see :func:`torch.set_default_dtype`).\n",
      "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "        Default: ``torch.strided``.\n",
      "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "        Default: if ``None``, uses the current device for the default tensor type\n",
      "        (see :func:`torch.set_default_device`). :attr:`device` will be the CPU\n",
      "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "    requires_grad (bool, optional): If autograd should record operations on the\n",
      "        returned tensor. Default: ``False``.\n",
      "    pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "        the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.randn(4)\n",
      "    tensor([-2.1436,  0.9966,  2.3426, -0.6366])\n",
      "    >>> torch.randn(2, 3)\n",
      "    tensor([[ 1.5954,  2.8929, -1.0923],\n",
      "            [ 1.1719, -0.4709, -0.1996]])\n",
      "\n",
      ".. _complex normal distribution: https://en.wikipedia.org/wiki/Complex_normal_distribution\n",
      "\u001b[31mType:\u001b[39m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "torch.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357637a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
