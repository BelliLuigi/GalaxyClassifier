{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c831bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torch.utils.data\n",
    "#from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f15060",
   "metadata": {},
   "source": [
    "### Define Dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cac3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyJungle(Dataset):\n",
    "    \n",
    "    #the init function initializes the directory containing the image,\n",
    "    #the annotations file,\n",
    "    #and both transforms\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None, is_rgb=False):\n",
    "        self.rgb = is_rgb\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    #returns number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return (self.img_labels).shape[0]\n",
    "\n",
    "    #loads a sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0])) + '.jpg'\n",
    "        #retrieves the image\n",
    "        image = Image.open(img_path)\n",
    "        if not self.rgb: image = image.convert('L')\n",
    "        #retrieves corresponding label\n",
    "        label = self.img_labels.iloc[idx, 1:]\n",
    "        #if possible, transform the image and the label into a tensor.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label, self.img_labels.iloc[idx, 0]\n",
    "    \n",
    "\n",
    "transfs = transforms.Compose([\n",
    "    transforms.ToTensor(), # Riscala le immagini tra 0 e 1\n",
    "    # sarebbe interessante implementare un random crop prima del center crop per decentrare un poco le immagini????\n",
    "    transforms.RandomHorizontalFlip(), # horizontal flip\n",
    "    transforms.RandomVerticalFlip(), # vertical flip\n",
    "    transforms.CenterCrop(324)          #CROP\n",
    "    ]) #transforms.compose per fare una pipe di transformazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0592f53",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d162ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, num_filters, num_neurons1, num_neurons2, activation, is_rgb=False, verbose=False):\n",
    "        super().__init__()\n",
    "        rgb = 3 if is_rgb else 1\n",
    "        input_size = 324\n",
    "        num_labels = 37\n",
    "        self.loss_dict = {'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "        self.activation = activation\n",
    "        self.num_convs = n_conv_layers\n",
    "\n",
    "\n",
    "        stride = 2\n",
    "        kernel_size = 3\n",
    "        kernel_size_pool = 2\n",
    "        \n",
    "        ## convolutional layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(rgb, num_filters[0], kernel_size=kernel_size, stride=stride),\n",
    "            nn.BatchNorm2d(num_filters[0])\n",
    "            ])\n",
    "        output_size = (input_size - kernel_size + stride) // (stride*kernel_size_pool)\n",
    "\n",
    "        if verbose: print('output size after first conv layer: ', output_size)\n",
    "\n",
    "        for i in range(1,n_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=kernel_size, stride=stride)) # num filters are the number of channel the conv layer outputs.\n",
    "            self.convs.append(nn.BatchNorm2d(num_filters[i]))\n",
    "            output_size = (output_size - kernel_size + stride) // (stride*kernel_size_pool) #padding 0, dilation = 1\n",
    "            if verbose: \n",
    "                if i != n_conv_layers - 1: print('output size after conv layer {}: '.format(i), output_size)\n",
    "                else: \n",
    "                    print('output size of the last conv layer: ', output_size)\n",
    "                    print('len self convs: ',len(self.convs))\n",
    "        if output_size in (0, -1): output_size = 1\n",
    "        print(output_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size_pool)\n",
    "        #self.convs.append(nn.dropout(p= value)) ## to be added in the future to test claims of BatchnOrm paper\n",
    "        \n",
    "        self.out_feature = num_filters[-1] *output_size * output_size # output size of the last conv layer, should be 38\n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons1) # fully connected layer\n",
    "        # dropout here if you want\n",
    "        self.fc2 = nn.Linear(num_neurons1, num_neurons2)\n",
    "        #dropout here if u want\n",
    "        self.fc3 = nn.Linear(num_neurons2, num_labels)\n",
    "        \n",
    "\n",
    "    def init_weights(self):\n",
    "        if self.activation == 'ReLU': # perchÃ¨ kaiming normal e non uniform??1\n",
    "            nonlin = 'relu'\n",
    "            for i in range(0, self.num_convs*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity=nonlin)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=nonlin)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=nonlin)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0)\n",
    "\n",
    "        elif self.activation == 'LeakyReLU':\n",
    "            nonlin = 'leaky_relu'\n",
    "            for i in range(0, self.num_convs*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, a = 0.01, nonlinearity=nonlin)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, a = 0.01, nonlinearity=nonlin)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, a = 0.01, nonlinearity=nonlin)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0) \n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.constant_(self.fc3.bias,0)\n",
    "        return print('weights initialized with {}'.format(self.activation))         \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        verbose=False\n",
    "        for i in range(0, len(self.convs),2):\n",
    "            x = self.convs[i](x)  # conv\n",
    "            x = self.activation(x)  # act\n",
    "            x = self.convs[i+1](x)  # batch norm\n",
    "            x = self.pool(x)  # pool\n",
    "            if verbose: print(x.shape)\n",
    "        x = torch.flatten(x,1) # flatten operation -> 1 dimensional\n",
    "        if verbose: print('last conv layer flattened',x.shape)\n",
    "        if verbose: print('out-feature: ',self.out_feature)\n",
    "        x = self.activation(self.fc1(x)) # apply relu al'output dei fully connected\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.activation(self.fc2(x)) # idem sopra\n",
    "        if verbose: print(x.shape)\n",
    "        x = self.fc3(x)\n",
    "        # x = nn.Sigmoid()(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        verbose=False\n",
    "        train = self.__getstate__()['training']\n",
    "        if verbose: print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efe364",
   "metadata": {},
   "source": [
    "img, lab, indx = DS.__getitem__(0)\n",
    "#print(lab)\n",
    "#print(img)         #3D TENSOR    \n",
    "if DS.rgb:\n",
    "    fig, ax = plt.subplots(1,3, figsize=(24,7))\n",
    "    color = ['Reds', 'Greens', 'Blues']\n",
    "    for i,j in enumerate(img):\n",
    "        ax[i].imshow(j, cmap=color[i])\n",
    "else:\n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,7))\n",
    "    ax.imshow(img[0], cmap='magma')\n",
    "#print(img.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357637a",
   "metadata": {},
   "source": [
    "## TRAINING + VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aedbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    model.train()\n",
    "    for i, data in tqdm(enumerate(train_loader)):\n",
    "        inputs,labels, _ = data\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs) #, activation=F.relu)\n",
    "        loss=loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # fa update del parameter\n",
    "        RMSEloss = np.sqrt(loss.item())\n",
    "        running_loss += RMSEloss\n",
    "        if verbose and i%10 ==0: print(f'Batch {i+1}/{len(train_loader)} - Loss: {RMSEloss:.3f}')\n",
    "\n",
    "        model.log_the_loss(RMSEloss, epoch=False)\n",
    "    epochmean_loss = running_loss / len(train_loader)\n",
    "    print(f'\\nLoss: {epochmean_loss:.3f}')\n",
    "    model.log_the_loss(epochmean_loss, epoch=True)\n",
    "    last_loss = RMSEloss\n",
    "    print(f\"Last loss: {last_loss:.3f}\")\n",
    "    return epochmean_loss\n",
    "\n",
    "\n",
    "\n",
    "def one_epoch_eval(model, test_loader, loss_function, verbose=False):\n",
    "    model.eval()\n",
    "    running_validation_loss = 0.\n",
    "   \n",
    "    with torch.no_grad(): # deactivates gradient evaluation\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            inputs,labels, _ = vdata\n",
    "            inputs,labels= inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)#, activation=F.relu)\n",
    "            loss = loss_function(outputs,labels)\n",
    "            RMSEloss = np.sqrt(loss.item())\n",
    "            running_validation_loss +=RMSEloss\n",
    "            model.log_the_loss(RMSEloss,epoch=False)\n",
    "    mean_vloss=model.log_the_loss(running_validation_loss/len(test_loader),epoch=True)\n",
    "    if verbose: print(f\"Validation Loss: {mean_vloss:.3f}\\n---\")\n",
    "    return mean_vloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590107b",
   "metadata": {},
   "source": [
    "## OPTUNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3343a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = GalaxyJungle('../data/training/training_solutions_rev1.csv', '../data/training/', transfs)\n",
    "training, test, true_test = random_split(DS, [.65, .2, .15])\n",
    "\n",
    "def objective(trial:optuna.Trial):\n",
    "   \n",
    "    ## Hyperspace\n",
    "    num_conv_layers = 3\n",
    "    #qui tuniamo il numero di filri, per layer piÃ¹ profondi ci vogliono piÃ¹ filtri (64-28 Ã¨ consigliato per pattern astratti e combinazioni, mentre fino a 32 per dettagli locali) quindi proviamo (VGG usa fino a 512 per esempio).\n",
    "    num_filters = [int(trial.suggest_int(\"num_filters_\"+str(i), 16, 128, step=16)) for i in range(num_conv_layers)]\n",
    "    ## abbiamo numneurons1 e numn neurons2,se mettiamo un grid sampler o un random sampler con num_neurons e basta penso che lui provi diverse combinazioni\n",
    "    num_neurons1 = trial.suggest_int(\"num_neurons\",50,200,step=10) \n",
    "    num_neurons2 = trial.suggest_int(\"num_neurons2\",10,150,step=10)\n",
    "    ### abbiamo chiamato mode l'activation function nell'initialization dei pesi o la chiamiamo activation o FUNZIONEDIATTIVAZIONE cosÃ¬ optuna poi iniializza in base a quello\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"ReLU\", \"LeakyReLU\"])\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"]) #AdamW Ã¨ suggerito per CNN.\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True) #log true cerca i valori in scala logaritmica\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.5, 0.9, step=0.1) #per SGD\n",
    "    # batch size da tunare?\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    ##### Training phase\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=8) \n",
    "\n",
    "    activation = getattr(nn, activation)()\n",
    "    model = GalaxyNet(num_conv_layers, num_filters, num_neurons1, num_neurons2, activation).to(device)\n",
    "    if optimizer == 'SGD': optimizer = getattr(optim, optimizer)(model.parameters(), lr=learning_rate, momentum = momentum)\n",
    "    else: optimizer = getattr(optim, optimizer)(model.parameters(), lr=learning_rate)\n",
    "    model.init_weights()\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        print(f'Training epoch {epoch}')\n",
    "        one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False)\n",
    "        print(f'Validation epoch {epoch}')\n",
    "        epoch_last_val_loss = one_epoch_eval(model, test_loader, loss_function, verbose=True)\n",
    "        trial.report(epoch_last_val_loss,epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    \n",
    "    score = epoch_last_val_loss\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9b544c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-19 12:01:51,961] Using an existing study with name 'first' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an existing study with name 'first' instead of creating a new one.\n",
      "Using an existing study with name 'first' instead of creating a new one.\n",
      "Using an existing study with name 'first' instead of creating a new one.\n",
      "4\n",
      "weights initialized with LeakyReLU(negative_slope=0.01)\n",
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:43, 28.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.144\n",
      "Last loss: 0.131\n",
      "Validation epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.128\n",
      "---\n",
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:44, 28.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.126\n",
      "Last loss: 0.142\n",
      "Validation epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.128\n",
      "---\n",
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:43, 28.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.120\n",
      "Last loss: 0.114\n",
      "Validation epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.118\n",
      "---\n",
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:45, 27.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.116\n",
      "Last loss: 0.109\n",
      "Validation epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.113\n",
      "---\n",
      "Training epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:43, 28.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.114\n",
      "Last loss: 0.114\n",
      "Validation epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.116\n",
      "---\n",
      "Training epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [01:33, 13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.112\n",
      "Last loss: 0.123\n",
      "Validation epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.116\n",
      "---\n",
      "Training epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [04:32,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.111\n",
      "Last loss: 0.114\n",
      "Validation epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.112\n",
      "---\n",
      "Training epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [04:32,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.110\n",
      "Last loss: 0.100\n",
      "Validation epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.110\n",
      "---\n",
      "Training epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:43, 28.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.109\n",
      "Last loss: 0.089\n",
      "Validation epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.113\n",
      "---\n",
      "Training epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:37, 33.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.108\n",
      "Last loss: 0.103\n",
      "Validation epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.112\n",
      "---\n",
      "Training epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:38, 32.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.107\n",
      "Last loss: 0.106\n",
      "Validation epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.111\n",
      "---\n",
      "Training epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:39, 31.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.106\n",
      "Last loss: 0.103\n",
      "Validation epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.110\n",
      "---\n",
      "Training epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:40, 31.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.106\n",
      "Last loss: 0.091\n",
      "Validation epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.108\n",
      "---\n",
      "Training epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:39, 31.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.105\n",
      "Last loss: 0.103\n",
      "Validation epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.106\n",
      "---\n",
      "Training epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:40, 30.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.105\n",
      "Last loss: 0.095\n",
      "Validation epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.107\n",
      "---\n",
      "Training epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:40, 30.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.104\n",
      "Last loss: 0.111\n",
      "Validation epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.107\n",
      "---\n",
      "Training epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1251it [00:46, 26.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.103\n",
      "Last loss: 0.105\n",
      "Validation epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.108\n",
      "---\n",
      "Training epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "259it [00:11, 22.57it/s]\n",
      "[W 2025-05-19 12:27:06,196] Trial 13 failed with parameters: {'num_filters_0': 16, 'num_filters_1': 128, 'num_filters_2': 128, 'num_neurons': 130, 'num_neurons2': 80, 'activation': 'LeakyReLU', 'optimizer': 'Adam', 'learning_rate': 0.0002738011654544197, 'momentum': 0.5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teobaldo/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20210/3746711845.py\", line 38, in objective\n",
      "    one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False)\n",
      "  File \"/tmp/ipykernel_20210/2758126940.py\", line 7, in one_epoch_train\n",
      "    inputs,labels = inputs.to(device), labels.to(device)\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 failed with parameters: {'num_filters_0': 16, 'num_filters_1': 128, 'num_filters_2': 128, 'num_neurons': 130, 'num_neurons2': 80, 'activation': 'LeakyReLU', 'optimizer': 'Adam', 'learning_rate': 0.0002738011654544197, 'momentum': 0.5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teobaldo/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20210/3746711845.py\", line 38, in objective\n",
      "    one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False)\n",
      "  File \"/tmp/ipykernel_20210/2758126940.py\", line 7, in one_epoch_train\n",
      "    inputs,labels = inputs.to(device), labels.to(device)\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Trial 13 failed with parameters: {'num_filters_0': 16, 'num_filters_1': 128, 'num_filters_2': 128, 'num_neurons': 130, 'num_neurons2': 80, 'activation': 'LeakyReLU', 'optimizer': 'Adam', 'learning_rate': 0.0002738011654544197, 'momentum': 0.5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teobaldo/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20210/3746711845.py\", line 38, in objective\n",
      "    one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False)\n",
      "  File \"/tmp/ipykernel_20210/2758126940.py\", line 7, in one_epoch_train\n",
      "    inputs,labels = inputs.to(device), labels.to(device)\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Trial 13 failed with parameters: {'num_filters_0': 16, 'num_filters_1': 128, 'num_filters_2': 128, 'num_neurons': 130, 'num_neurons2': 80, 'activation': 'LeakyReLU', 'optimizer': 'Adam', 'learning_rate': 0.0002738011654544197, 'momentum': 0.5} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/teobaldo/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20210/3746711845.py\", line 38, in objective\n",
      "    one_epoch_train(model, train_loader, optimizer, loss_function, verbose=False)\n",
      "  File \"/tmp/ipykernel_20210/2758126940.py\", line 7, in one_epoch_train\n",
      "    inputs,labels = inputs.to(device), labels.to(device)\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-19 12:27:06,199] Trial 13 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 failed with value None.\n",
      "Trial 13 failed with value None.\n",
      "Trial 13 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m storage_name = \u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.db\u001b[39m\u001b[33m\"\u001b[39m.format(study_name)\n\u001b[32m      4\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m, study_name=study_name, storage=storage_name, load_if_exists=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m study.optimize(objective, n_trials=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     _optimize(\n\u001b[32m    476\u001b[39m         study=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    477\u001b[39m         func=func,\n\u001b[32m    478\u001b[39m         n_trials=n_trials,\n\u001b[32m    479\u001b[39m         timeout=timeout,\n\u001b[32m    480\u001b[39m         n_jobs=n_jobs,\n\u001b[32m    481\u001b[39m         catch=\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[32m    482\u001b[39m         callbacks=callbacks,\n\u001b[32m    483\u001b[39m         gc_after_trial=gc_after_trial,\n\u001b[32m    484\u001b[39m         show_progress_bar=show_progress_bar,\n\u001b[32m    485\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         _optimize_sequential(\n\u001b[32m     64\u001b[39m             study,\n\u001b[32m     65\u001b[39m             func,\n\u001b[32m     66\u001b[39m             n_trials,\n\u001b[32m     67\u001b[39m             timeout,\n\u001b[32m     68\u001b[39m             catch,\n\u001b[32m     69\u001b[39m             callbacks,\n\u001b[32m     70\u001b[39m             gc_after_trial,\n\u001b[32m     71\u001b[39m             reseed_sampler_rng=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m             time_start=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m             progress_bar=progress_bar,\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = _run_trial(study, func, catch)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jungle/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = func(trial)\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, epochs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     one_epoch_train(model, train_loader, optimizer, loss_function, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mValidation epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m     epoch_last_val_loss = one_epoch_eval(model, test_loader, loss_function, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mone_epoch_train\u001b[39m\u001b[34m(model, train_loader, optimizer, loss_function, verbose)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[32m      6\u001b[39m     inputs,labels, _ = data\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     inputs,labels = inputs.to(device), labels.to(device)\n\u001b[32m      8\u001b[39m     optimizer.zero_grad()\n\u001b[32m      9\u001b[39m     outputs = model(inputs) \u001b[38;5;66;03m#, activation=F.relu)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study_name = \"first\"  # Unique identifier of the study.\n",
    "storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "study = optuna.create_study(direction='minimize', study_name=study_name, storage=storage_name, load_if_exists=True)\n",
    "study.optimize(objective, n_trials=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jungle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
