{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c831bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/galzoo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "#from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cac3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyJungle(Dataset): # sarebbe interessante implementare un rescale/crop\n",
    "    \n",
    "    #the init function initializes the directory containing the image,\n",
    "    #the annotations file,\n",
    "    #and both transforms\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "\n",
    "    #returns number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return (self.img_labels).shape[0]\n",
    "\n",
    "    #loads a sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, str(self.img_labels.iloc[idx, 0])) + '.jpg'\n",
    "        #retrieves the image\n",
    "        image = Image.open(img_path)\n",
    "        if prova.set_rgb: image = image.convert('L')\n",
    "        #retrieves corresponding label\n",
    "        label = self.img_labels.iloc[idx, 1:]\n",
    "        #if possible, transform the image and the label into a tensor.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(label.values, dtype=torch.float32)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label, self.img_labels.iloc[idx, 0]\n",
    "    \n",
    "\n",
    "transfs = transforms.Compose([\n",
    "    transforms.ToTensor(), #fa già la normalizzazione se l'immagine non è un tensore\n",
    "    # sarebbe interessante implementare un random crop prima del center crop per decentrare un poco le immagini????\n",
    "    transforms.RandomHorizontalFlip(), # horizontal flip\n",
    "    transforms.RandomVerticalFlip(), # vertical flip\n",
    "    transforms.CenterCrop(324)          #CROP\n",
    "    ]) #transforms.compose per fare una pipe di transformazioni\n",
    "\n",
    "\n",
    "\n",
    "DS = GalaxyJungle('../data/training/training_solutions_rev1.csv', '../data/training/', transfs)\n",
    "training, test = random_split(DS, [.8, .2])\n",
    "train_loader = DataLoader(training, batch_size=(batch_size_train := 128), shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test, batch_size=(batch_size_test := 128), shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d162ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalaxyNet(nn.Module):\n",
    "    def __init__(self, n_conv_layers, num_filters,num_neurons1, num_neurons2):\n",
    "        super().__init__()\n",
    "        self.rgb = 1\n",
    "        self.input_size = 324\n",
    "        self.num_labels = 37\n",
    "        self.loss_dict = { 'batch' : [], 'epoch' : [], 'vbatch' : [], 'vepoch' : []}\n",
    "\n",
    "        verbose=True\n",
    "        stride = 2\n",
    "        kernel_size = 3\n",
    "        kernel_size_pool = 2\n",
    "        ## convolutional layers\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(self.rgb, num_filters[0], kernel_size=kernel_size, stride=stride)])\n",
    "        self.convs.append(nn.BatchNorm2d(num_filters[0]))\n",
    "        output_size = (self.input_size - kernel_size + stride) // (stride*kernel_size_pool)\n",
    "        if verbose: print('output size after first conv layer: ', output_size)\n",
    "        for i in range(1,n_conv_layers):\n",
    "            self.convs.append(nn.Conv2d(num_filters[i-1], num_filters[i], kernel_size=kernel_size, stride=stride)) # num filters are the number of channel the conv layer outputs.\n",
    "            self.convs.append(nn.BatchNorm2d(num_filters[i]))\n",
    "            output_size = (output_size - kernel_size + stride) // (stride*kernel_size_pool) #padding 0, dilation = 1\n",
    "            if verbose: print('output size after conv layer {}: '.format(i), output_size)\n",
    "        if output_size < 1:\n",
    "            raise ValueError(f\"Output size too small after {i+1} conv layers. Reduce n_conv_layers or adjust kernel/stride/pool.\")\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size_pool)\n",
    "        #self.convs.append(nn.dropout(p= value)) ## to be added in the future to test claims of BatchnOrm paper\n",
    "        self.out_feature = num_filters[-1] *output_size * output_size # output size of the last conv layer, should be 38\n",
    "        if verbose: print('output size of the last conv layer: ', output_size)\n",
    "        if verbose: print('len self convs: ',len(self.convs))\n",
    "        #self.out_feature= num_filters[-1] * \n",
    "        self.fc1 = nn.Linear(self.out_feature, num_neurons1) # fully connected layer\n",
    "        # dropout here if you want\n",
    "        self.fc2 = nn.Linear(num_neurons1, num_neurons2)\n",
    "        #dropout here if u want\n",
    "        self.fc3 = nn.Linear(num_neurons2, self.num_labels)\n",
    "\n",
    "    def init_weights(self,  mode, n_conv_layers):\n",
    "        if mode == 'relu': # perchè kaiming normal e non uniform??1\n",
    "            for i in range(0, n_conv_layers*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, nonlinearity=mode)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=mode)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=mode)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0)\n",
    "        elif mode == 'leaky_relu':\n",
    "            for i in range(0, n_conv_layers*2, 2):\n",
    "                nn.init.kaiming_normal_(self.convs[i].weight, a = 0.01, nonlinearity=mode)\n",
    "                if self.convs[i].bias is not None:\n",
    "                    nn.init.constant_(self.convs[i].bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc1.weight, nonlinearity=mode)\n",
    "            if self.fc1.bias is not None:\n",
    "                nn.init.constant_(self.fc1.bias,0)\n",
    "            nn.init.kaiming_normal_(self.fc2.weight, nonlinearity=mode)\n",
    "            if self.fc2.bias is not None:\n",
    "                nn.init.constant_(self.fc2.bias,0) \n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.constant_(self.fc3.bias,0)\n",
    "        return print('weights initialized with {}'.format(mode))        \n",
    "        \n",
    "\n",
    "    def forward(self,x, activation):\n",
    "        verbose=False\n",
    "        for i in range(0, len(self.convs),2):\n",
    "            x = self.pool(activation(self.convs[i](x)))\n",
    "            x = self.convs[i+1](x) # batch norm layer\n",
    "            if verbose: print(x.shape)\n",
    "        x = torch.flatten(x,1) # flatten operation -> 1 dimensional\n",
    "        if verbose: print('last conv layer flattened',x.shape)\n",
    "        if verbose: print('out-feature: ',self.out_feature)\n",
    "        x = activation(self.fc1(x)) # apply relu al'output dei fully connected\n",
    "        if verbose: print(x.shape)\n",
    "        x = activation(self.fc2(x)) # idem sopra\n",
    "        if verbose: print(x.shape)\n",
    "        x = F.sigmoid(self.fc3(x)) # output di fc3, 37 neuroni -> 37 classi ideally \n",
    "        ## MAPPING ALSO HERE\n",
    "        return x\n",
    "    \n",
    "    def set_rgb(self,bool):\n",
    "        if bool:\n",
    "            self.rgb = 3\n",
    "        else:\n",
    "            self.rgb = 1\n",
    "        return self.rgb\n",
    "    \n",
    "    def log_the_loss(self, item,epoch=False): # per avere una history della loss???\n",
    "        verbose=False\n",
    "        train = self.__getstate__()['training']\n",
    "        if verbose: print(train)\n",
    "        if epoch and train:\n",
    "            self.loss_dict['epoch'].append(item) ### get state of the model so you can ditch the validation parameter\n",
    "        elif not epoch and train:\n",
    "            self.loss_dict['batch'].append(item)\n",
    "        elif not train and epoch:\n",
    "            self.loss_dict['vepoch'].append(item)\n",
    "        elif not train and not epoch:\n",
    "            self.loss_dict['vbatch'].append(item)\n",
    "        return item\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06e03fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size after first conv layer:  80\n",
      "output size after conv layer 1:  19\n",
      "output size of the last conv layer:  19\n",
      "len self convs:  4\n"
     ]
    }
   ],
   "source": [
    "#n_conv_layers, num_filters,num_neurons1, num_neurons2):\n",
    "prova = GalaxyNet(2,[3,6],50, 45).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44d75c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova.set_rgb(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357637a",
   "metadata": {},
   "source": [
    "## TRAINING + VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fad1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.SGD(prova.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8aedbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_train(modelname,epoch,activation,verbose=False):\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    modelname.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs,labels, _ = data\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = modelname(inputs, activation)\n",
    "        loss=loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() # fa update del parameter\n",
    "        RMSEloss = np.sqrt(loss.item()) #provare con torch\n",
    "        running_loss += RMSEloss\n",
    "        if verbose and i%10 ==0: print(f'Batch {i+1}/{len(train_loader)} - Loss: {RMSEloss:.3f}')\n",
    "\n",
    "        modelname.log_the_loss(RMSEloss, epoch=False)\n",
    "        if i == len(train_loader)-1: #se siamo alla fine del ciclo\n",
    "            epochmean_loss = running_loss / len(train_loader)\n",
    "            if verbose: print(f'--- \\nEnd of epoch - Loss: {epochmean_loss:.3f}')\n",
    "            modelname.log_the_loss(epochmean_loss, epoch=True)\n",
    "            last_loss = RMSEloss\n",
    "            if verbose:print(f\"\\nEpoch {epoch} - Last loss: {last_loss:.3f}\\n---\")\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a8f516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch_validation(modelname,epoch,activation, verbose= False):\n",
    "    modelname.eval()\n",
    "    running_val_loss = 0.\n",
    "    if verbose: print('Validation phase')\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_loader):\n",
    "            inputs, labels, _ = vdata\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = modelname(inputs, activation)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            RMSEloss = np.sqrt(loss.item())\n",
    "            running_val_loss += RMSEloss\n",
    "            if verbose and i%10 ==0: print(f'Batch {i+1}/{len(test_loader)} - Loss: {RMSEloss:.3f}')\n",
    "            modelname.log_the_loss(RMSEloss, epoch=False)\n",
    "            if i == len(test_loader)-1:\n",
    "                epochmean_val_loss = running_val_loss / len(test_loader)\n",
    "                if verbose: print(f'--- \\nEnd of validation - Loss: {epochmean_val_loss:.3f}')\n",
    "                modelname.log_the_loss(epochmean_val_loss, epoch=True)\n",
    "                last_val_loss = RMSEloss\n",
    "                if verbose:print(f\"\\nValidation epoch {epoch} - Last loss: {last_val_loss:.3f}\\n---\")\n",
    "    return last_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcd76083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "   \n",
    "    ## Hyperspace\n",
    "    num_conv_layers = trial.suggest_int(\"num_conv_layers\", 2, 4,step= 1)\n",
    "    #qui tuniamo il numero di filri, per layer più profondi ci vogliono più filtri (64-28 è consigliato per pattern astratti e combinazioni, mentre fino a 32 per dettagli locali) quindi proviamo (VGG usa fino a 512 per esempio).\n",
    "    num_filters = [int(trial.suggest_int(\"num_filters_\"+str(i), 16, 128, step=16)) for i in range(num_conv_layers)]\n",
    "    ## abbiamo numneurons1 e numn neurons2,se mettiamo un grid sampler o un random sampler con num_neurons e basta penso che lui provi diverse combinazioni\n",
    "    num_neurons1 = trial.suggest_int(\"num_neurons\",50,200,step=10) \n",
    "    num_neurons2 = trial.suggest_int(\"num_neurons2\",10,150,step=10)\n",
    "    ### abbiamo chiamato mode l'activation function nell'initialization dei pesi o la chiamiamo activation o FUNZIONEDIATTIVAZIONE così optuna poi iniializza in base a quello\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\"])\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"]) #AdamW è suggerito per CNN.\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True) #log true cerca i valori in scala logaritmica\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.5, 0.9, step=0.1) #per SGD\n",
    "    # batch size da tunare?\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 32, 256, step=32)\n",
    "    epochs = trial.suggest_int(\"epochs\", 5, 45,step= 5)\n",
    "    \n",
    "    \n",
    "    ##### Training phase\n",
    "    \n",
    "    DS = GalaxyJungle('../data/training/training_solutions_rev1.csv', '../data/training/', transfs)\n",
    "    training, test = random_split(DS, [.8, .2])\n",
    "    train_loader = DataLoader(training, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=8) \n",
    "\n",
    "    model = GalaxyNet(num_conv_layers, num_filters, num_neurons1, num_neurons2).to(device)\n",
    "\n",
    "    if activation == 'relu':\n",
    "        activation = F.relu\n",
    "    elif activation == 'leaky_relu':\n",
    "        activation = F.leaky_relu\n",
    "    \n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    elif optimizer == 'AdamW':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.init_weights(activation, num_conv_layers)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        print(f'Training epoch {epoch}')\n",
    "        epoch_last_loss = one_epoch_train(model,epoch,activation, verbose=False)\n",
    "        print(f'Validation epoch {epoch}')\n",
    "        epoch_last_val_loss = one_epoch_validation(prova,epoch,activation, verbose=False)\n",
    "    \n",
    "    score = epoch_last_val_loss\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d63ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 11:02:51,739] A new study created in memory with name: no-name-65661df6-cc2f-4b55-8155-47caac286f6f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size after first conv layer:  80\n",
      "output size after conv layer 1:  19\n",
      "output size of the last conv layer:  19\n",
      "len self convs:  4\n",
      "weights initialized with <function relu at 0x7c79d4010180>\n",
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-17 11:03:32,158] Trial 0 failed with parameters: {'num_conv_layers': 2, 'num_filters_0': 96, 'num_filters_1': 48, 'num_neurons': 60, 'num_neurons2': 140, 'activation': 'relu', 'optimizer': 'SGD', 'learning_rate': 0.0001555864048712945, 'momentum': 0.7, 'batch_size': 256, 'epochs': 20} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_250033/3740889451.py\", line 45, in objective\n",
      "    epoch_last_loss = one_epoch_train(model,epoch,activation, verbose=False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_250033/1484368393.py\", line 11, in one_epoch_train\n",
      "    loss.backward()\n",
      "  File \"/home/ubuntu/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/_tensor.py\", line 525, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/ubuntu/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 267, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/ubuntu/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-17 11:03:32,161] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m study.optimize(objective, n_trials=\u001b[32m100\u001b[39m, timeout=\u001b[32m3600\u001b[39m*\u001b[32m10\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     _optimize(\n\u001b[32m    476\u001b[39m         study=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    477\u001b[39m         func=func,\n\u001b[32m    478\u001b[39m         n_trials=n_trials,\n\u001b[32m    479\u001b[39m         timeout=timeout,\n\u001b[32m    480\u001b[39m         n_jobs=n_jobs,\n\u001b[32m    481\u001b[39m         catch=\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[32m    482\u001b[39m         callbacks=callbacks,\n\u001b[32m    483\u001b[39m         gc_after_trial=gc_after_trial,\n\u001b[32m    484\u001b[39m         show_progress_bar=show_progress_bar,\n\u001b[32m    485\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         _optimize_sequential(\n\u001b[32m     64\u001b[39m             study,\n\u001b[32m     65\u001b[39m             func,\n\u001b[32m     66\u001b[39m             n_trials,\n\u001b[32m     67\u001b[39m             timeout,\n\u001b[32m     68\u001b[39m             catch,\n\u001b[32m     69\u001b[39m             callbacks,\n\u001b[32m     70\u001b[39m             gc_after_trial,\n\u001b[32m     71\u001b[39m             reseed_sampler_rng=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     72\u001b[39m             time_start=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m             progress_bar=progress_bar,\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = _run_trial(study, func, catch)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = func(trial)\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, epochs):\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     epoch_last_loss = one_epoch_train(model,epoch,activation, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mValidation epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     47\u001b[39m     epoch_last_val_loss = one_epoch_validation(prova,epoch,activation, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mone_epoch_train\u001b[39m\u001b[34m(modelname, epoch, activation, verbose)\u001b[39m\n\u001b[32m      9\u001b[39m outputs = modelname(inputs, activation)\n\u001b[32m     10\u001b[39m loss=loss_function(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m loss.backward()\n\u001b[32m     12\u001b[39m optimizer.step() \u001b[38;5;66;03m# fa update del parameter\u001b[39;00m\n\u001b[32m     13\u001b[39m RMSEloss = np.sqrt(loss.item()) \u001b[38;5;66;03m#provare con torch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    517\u001b[39m         Tensor.backward,\n\u001b[32m    518\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m         inputs=inputs,\n\u001b[32m    524\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m torch.autograd.backward(\n\u001b[32m    526\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    527\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    262\u001b[39m     retain_graph = create_graph\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m _engine_run_backward(\n\u001b[32m    268\u001b[39m     tensors,\n\u001b[32m    269\u001b[39m     grad_tensors_,\n\u001b[32m    270\u001b[39m     retain_graph,\n\u001b[32m    271\u001b[39m     create_graph,\n\u001b[32m    272\u001b[39m     inputs,\n\u001b[32m    273\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    274\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    275\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/galzoo/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m744\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    745\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    746\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "study.optimize(objective, n_trials=100, timeout=3600*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888792e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galzoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
